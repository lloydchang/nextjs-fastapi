# .env.example

# Amazon Bedrock Titan Configuration
AMAZON_BEDROCK_TITAN_TEXT_MODEL=amazon.titan-text-express-v1
AMAZON_BEDROCK_TITAN_EMBEDDING_MODEL=amazon.titan-embed-text-v2:0
AMAZON_BEDROCK_TITAN_ENDPOINT=your-amazon-bedrock-titan-endpoint

# Azure OpenAI O1
AZURE_OPENAI_O1_TEXT_MODEL=o1-mini
AZURE_OPENAI_O1_EMBEDDING_MODEL=text-embedding-3-small
AZURE_OPENAI_O1_ENDPOINT=your-azure-openai-o1-endpoint
AZURE_OPENAI_O1_API_KEY=your-azure-openai-o1-api-key

# Cloudflare Gemma Configuration
CLOUDFLARE_GEMMA_TEXT_MODEL=gemma2:2b
CLOUDFLARE_GEMMA_ENDPOINT=your-cloudflare-gemma-endpoint
CLOUDFLARE_API_KEY=your-cloudflare-api-key

# Google Vertex Gemini Configuration
GOOGLE_VERTEX_GEMINI_TEXT_MODEL=gemini-1.5-flash-8b
GOOGLE_VERTEX_GEMINI_EMBEDDING_MODEL=text-embedding-004
GOOGLE_VERTEX_GEMINI_LOCATION=your-google-vertex-gemini-location
GOOGLE_APPLICATION_CREDENTIALS=your-google-application-credentials
GOOGLE_CLOUD_PROJECT=your-google-cloud-project-id

# Google Vertex Gemma Configuration
GOOGLE_VERTEX_GEMMA_TEXT_MODEL=gemma2:2b
GOOGLE_VERTEX_GEMMA_EMBEDDING_MODEL=text-embedding-004
GOOGLE_VERTEX_GEMMA_LOCATION=your-google-vertex-gemma-location

# Ollama Gemma Configuration
OLLAMA_GEMMA_TEXT_MODEL=gemma2:2b
OLLAMA_GEMMA_ENDPOINT=http://localhost:11434/api/generate

# Ollama Llama Configuration
OLLAMA_LLAMA_TEXT_MODEL=llama3.2:1b
OLLAMA_LLAMA_ENDPOINT=http://localhost:11434/api/generate

# OpenAI O1
OPENAI_O1_TEXT_MODEL=o1-mini
OPENAI_O1_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_O1_ENDPOINT=your-azure-openai-o1-endpoint
OPENAI_O1_API_KEY=your-openai-o1-api-key

# Enable/Disable Streaming Responses
STREAM_ENABLED=true

# Temperature for Model Generation
TEMPERATURE=0.0

# Set Winston Log Level
WINSTON_LOG_LEVEL=debug  # Change this to error, warn, info, http, verbose, debug, silly, etc. per https://github.com/winstonjs/winston?tab=readme-ov-file#logging-levels
