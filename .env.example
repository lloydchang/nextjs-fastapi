# .env.example

# Amazon Bedrock Titan Configuration
AMAZON_BEDROCK_TITAN_MODEL=your-amazon-bedrock-titan-model-id
AMAZON_BEDROCK_TITAN_ENDPOINT=http://localhost:3000/api/amazon/bedrock

# Google Vertex Configuration
GOOGLE_VERTEX_MODEL=your-google-vertex-model-id
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json
GOOGLE_VERTEX_LOCATION=your-google-vertex-location
GOOGLE_CLOUD_PROJECT=your-google-cloud-project-id
GOOGLE_VERTEX_ENDPOINT=http://localhost:3000/api/google/vertex

# Ollama Gemma Configuration
OLLAMA_GEMMA_MODEL=gemma2:2b
OLLAMA_GEMMA_ENDPOINT=http://localhost:11434/api/generate

# Ollama Llama Configuration
OLLAMA_LLAMA_MODEL=llama3.2:1b
OLLAMA_LLAMA_ENDPOINT=http://localhost:11435/api/generate

# Enable/Disable Streaming Responses
STREAM_ENABLED=true

# Temperature for Model Generation
TEMPERATURE=2.0

# Toggle Rate Limiting
RATE_LIMIT_ENABLED=false

# Enable/Disable Logs in Response
LOGS_IN_RESPONSE=false
