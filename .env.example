# .env.local

# Default Model
DEFAULT_MODEL=gemini-1.5-flash-8b
FALLBACK_MODEL=llama3.2

# Google Vertex AI Configuration
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json
GOOGLE_GEMINI_MODEL=your-google-gemini-model-id
GOOGLE_VERTEX_AI_LOCATION=your-google-vertex-ai-location
GOOGLE_CLOUD_PROJECT=your-google-cloud-project-id

# Ollama LLaMA Configuration
LOCAL_LLAMA_ENDPOINT=http://localhost:11434/api/generate

# Enable/Disable Streaming Responses
STREAM_ENABLED=true

# Temperature for Model Generation
TEMPERATURE=2.0

# Toggle Rate Limiting
RATE_LIMIT_ENABLED=false

# Enable/Disable Logs in Response
LOGS_IN_RESPONSE=false

