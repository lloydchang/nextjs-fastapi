# .env.example

# Amazon Bedrock Titan Configuration
AMAZON_BEDROCK_TITAN_MODEL=your-amazon-bedrock-titan-model-id
AMAZON_BEDROCK_TITAN_ENDPOINT=your-amazon-bedrock-titan-endpoint

# Google Vertex Configuration
GOOGLE_APPLICATION_CREDENTIALS=your-google-application-credentials
GOOGLE_CLOUD_PROJECT=your-google-cloud-project-id
GOOGLE_VERTEX_GEMINI_MODEL=your-google-vertex-gemini-model-id
GOOGLE_VERTEX_GEMINI_LOCATION=your-google-vertex-gemini-location

# Ollama Gemma Configuration
OLLAMA_GEMMA_MODEL=gemma2:2b
OLLAMA_GEMMA_ENDPOINT=http://localhost:11434/api/generate

# Ollama Llama Configuration
OLLAMA_LLAMA_MODEL=llama3.2:1b
OLLAMA_LLAMA_ENDPOINT=http://localhost:11434/api/generate

# Enable/Disable Streaming Responses
STREAM_ENABLED=true

# Temperature for Model Generation
TEMPERATURE=2.0

# Toggle Rate Limiting
RATE_LIMIT_ENABLED=false

# Enable/Disable Logs in Response
LOGS_IN_RESPONSE=false

# Set Winston Log Level
WINSTON_LOG_LEVEL=silly  # Change this to error, warn, info, http, verbose, debug, silly, etc. per https://github.com/winstonjs/winston?tab=readme-ov-file#logging-levels
