# File: .env.local.example

# Amazon Bedrock Titan Configuration
AMAZON_BEDROCK_TITAN_TEXT_MODEL=amazon.titan-text-express-v1
AMAZON_BEDROCK_TITAN_EMBEDDING_MODEL=amazon.titan-embed-text-v2:0
AMAZON_BEDROCK_TITAN_ENDPOINT=your-amazon-bedrock-titan-endpoint

# Azure OpenAI O1 Configuration
AZURE_OPENAI_O1_TEXT_MODEL=o1-mini
AZURE_OPENAI_O1_EMBEDDING_MODEL=text-embedding-3-small
AZURE_OPENAI_O1_ENDPOINT=your-azure-openai-o1-endpoint
AZURE_OPENAI_O1_API_KEY=your-azure-openai-o1-api-key

# Cloudflare Gemma Configuration
CLOUDFLARE_GEMMA_TEXT_MODEL=gemma-2b-it-lora
CLOUDFLARE_GEMMA_ACCOUNT_ID=your-cloudflare-gemma-account-id
CLOUDFLARE_GEMMA_ENDPOINT=https://api.cloudflare.com/client/v4/accounts/${CLOUDFLARE_GEMMA_ACCOUNT_ID}/ai/run/@cf/google/${CLOUDFLARE_GEMMA_TEXT_MODEL}
CLOUDFLARE_GEMMA_BEARER_TOKEN=your-cloudflare-gemma-x-bearer-token
CLOUDFLARE_GEMMA_X_AUTH_EMAIL=your-cloudflare-gemma-x-auth-email
CLOUDFLARE_GEMMA_X_AUTH_KEY=your-cloudflare-gemma-x-auth-key

# https://github.com/orgs/vercel/discussions/3621#discussioncomment-6740777
# Vercel does not have support in the Environment Variables UI
# to reference other variables. Referencing other variables
# is only supported in .env* files.
#
# # CLOUDFLARE_GEMMA_ENDPOINT=https://api.cloudflare.com/client/v4/accounts/${CLOUDFLARE_GEMMA_ACCOUNT_ID}/ai/run/@cf/google/${CLOUDFLARE_GEMMA_TEXT_MODEL}
# works in .env.local but not in Vercel's Environment Variables UI.

# Cloudflare Llama Configuration
CLOUDFLARE_LLAMA_TEXT_MODEL=llama-3.2-1b-instruct
CLOUDFLARE_LLAMA_ACCOUNT_ID=your-cloudflare-llama-account-id
CLOUDFLARE_LLAMA_ENDPOINT=https://api.cloudflare.com/client/v4/accounts/${CLOUDFLARE_LLAMA_ACCOUNT_ID}/ai/run/@cf/meta/${CLOUDFLARE_LLAMA_TEXT_MODEL}
CLOUDFLARE_LLAMA_BEARER_TOKEN=your-cloudflare-llama-x-bearer-token
CLOUDFLARE_LLAMA_X_AUTH_EMAIL=your-cloudflare-llama-x-auth-email
CLOUDFLARE_LLAMA_X_AUTH_KEY=your-cloudflare-llama-x-auth-key

# Google Vertex Gemini Configuration
GOOGLE_VERTEX_GEMINI_TEXT_MODEL=gemini-1.5-flash-8b
GOOGLE_VERTEX_GEMINI_EMBEDDING_MODEL=text-embedding-004
GOOGLE_VERTEX_GEMINI_LOCATION=your-google-vertex-gemini-location
GOOGLE_APPLICATION_CREDENTIALS=your-google-application-credentials
GOOGLE_CLOUD_PROJECT=your-google-cloud-project-id

# Google Vertex Gemma Configuration
GOOGLE_VERTEX_GEMMA_TEXT_MODEL=gemma2:2b
GOOGLE_VERTEX_GEMMA_EMBEDDING_MODEL=text-embedding-004
GOOGLE_VERTEX_GEMMA_LOCATION=your-google-vertex-gemma-location
GOOGLE_VERTEX_GEMMA_ENDPOINT=your-google-vertex-gemma-endpoint

# Google Vertex Llama Configuration
GOOGLE_VERTEX_LLAMA_TEXT_MODEL=llama3.2:1b
GOOGLE_VERTEX_LLAMA_EMBEDDING_MODEL=text-embedding-004
GOOGLE_VERTEX_LLAMA_LOCATION=your-google-vertex-llama-location
GOOGLE_VERTEX_LLAMA_ENDPOINT=your-google-vertex-llama-endpoint

# Ollama Gemma Configuration
OLLAMA_GEMMA_TEXT_MODEL=gemma2:2b
OLLAMA_GEMMA_ENDPOINT=http://localhost:11434/api/generate

# Ollama Llama Configuration
OLLAMA_LLAMA_TEXT_MODEL=llama3.2:1b
OLLAMA_LLAMA_ENDPOINT=http://localhost:11434/api/generate

# OpenAI O1 Configuration
OPENAI_O1_TEXT_MODEL=o1-mini
OPENAI_O1_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_O1_ENDPOINT=your-openai-o1-endpoint
OPENAI_O1_API_KEY=your-openai-o1-api-key

# Streaming Responses
STREAM=true

# Temperature for Model Generation
TEMPERATURE=0.0

# Set Winston Log Level
WINSTON_LOGGER_LEVEL=silly  # Change this to error, warn, info, http, verbose, debug, silly, etc. based on https://github.com/winstonjs/winston#logging-levels
